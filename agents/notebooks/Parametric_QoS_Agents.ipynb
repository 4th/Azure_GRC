{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55d88247",
   "metadata": {},
   "source": [
    "\n",
    "# Parametric QoS — Agents Notebook\n",
    "\n",
    "This notebook was generated from your Mermaid specification and contains the requested sections: **[SETUP]**, **[SETUP-ENV]**, **[KERNEL]**, **[TOOLS]**, **[AGENTS]**, **[WIRES]**, **[DEMO]**.\n",
    "\n",
    "```mermaid\n",
    "flowchart LR \n",
    "  %% ===================== PARAMETRIC (QoS) =====================\n",
    "  %% Latency / cost constraints and budgets per tool / per turn\n",
    "\n",
    "  %% -------- CONFIG (INTENT / TENANT) --------\n",
    "  subgraph CFG[\"QoS Config (per tenant, per intent)\"]\n",
    "    SLO_LAT[p95_latency_target_ms]\n",
    "    BUD_TOK[budget_tokens_per_turn]\n",
    "    BUD_USD[budget_us_per_turn]\n",
    "    CAP_TOOL[per_tool_caps\\n(tokens, timeouts, retries)]\n",
    "    PRICE[pricing_model\\n($/1k tokens, egress $)]\n",
    "  end\n",
    "\n",
    "  %% -------- TURN CONTEXT --------\n",
    "  subgraph TURN[\"Conversation Turn\"]\n",
    "    STEPS[planned_steps\\n(tool_i ... tool_n)]\n",
    "    META[trace_id, tenant_id,\\nintent_id, model_id]\n",
    "  end\n",
    "\n",
    "  %% -------- COST/LATENCY MODELS --------\n",
    "  subgraph MODELS[\"Cost & Latency Models\"]\n",
    "    CTOK[Token counter\\nprompt_i + completion_i]\n",
    "    CLAT[Latency model\\nsum(tool_i_latency) + overhead]\n",
    "    CUSD[Cost model\\nΣ(tokens_i/1000 * price_i)]\n",
    "  end\n",
    "\n",
    "  %% -------- CONSTRAINTS (PARAMETRIC) --------\n",
    "  subgraph CON[\"Constraints\"]\n",
    "    C1{{Latency constraint:\\n p95_latency <= SLO_LAT}}\n",
    "    C2{{Token constraint:\\n Σ tokens_i <= BUD_TOK}}\n",
    "    C3{{Cost constraint:\\n Σ cost_i <= BUD_USD}}\n",
    "    C4{{Per-tool caps:\\n tokens_i <= cap_i\\n retries_i <= rcap_i\\n latency_i <= lcap_i}}\n",
    "  end\n",
    "\n",
    "  %% -------- ENFORCEMENT / ACTIONS --------\n",
    "  subgraph ACT[\"Enforcement & Adaptation\"]\n",
    "    ENF[Enforce caps\\n(deny/degrade)]\n",
    "    ADAPT[Adapt plan\\n(cheaper model,\\ncombine steps,\\nreduce context)]\n",
    "    PASS[Pass gate]\n",
    "    FAIL[Fail gate / refuse]\n",
    "  end\n",
    "\n",
    "  %% ===================== WIRING =====================\n",
    "  %% Config feeds models and constraints\n",
    "  CFG --> MODELS\n",
    "  CFG --> CON\n",
    "\n",
    "  %% Turn feeds models\n",
    "  TURN --> MODELS\n",
    "  MODELS -->|tokens_i, latency_i, cost_i| CON\n",
    "\n",
    "  %% Constraint evaluation\n",
    "  C1 -->|ok| PASS\n",
    "  C2 -->|ok| PASS\n",
    "  C3 -->|ok| PASS\n",
    "  C4 -->|ok| PASS\n",
    "\n",
    "  C1 -->|violation| ENF\n",
    "  C2 -->|violation| ENF\n",
    "  C3 -->|violation| ENF\n",
    "  C4 -->|violation| ENF\n",
    "\n",
    "  ENF --> ADAPT -->|recompute| MODELS\n",
    "  PASS -->|execute tools| DONE[Proceed with turn]\n",
    "  ENF -->|cannot adapt| FAIL\n",
    "\n",
    "  %% ===================== NOTES =====================\n",
    "  %% Example equations (per turn):\n",
    "  %% - p95_latency = max(p95(tool_i)) + orchestration_overhead\n",
    "  %% - tokens_total = Σ (prompt_i + completion_i)\n",
    "  %% - cost_total = Σ ((tokens_i / 1000) * price_model_i) + egress_cost\n",
    "  %% - caps: {tokens_i, retries_i, latency_i} from CAP_TOOL; deny or degrade on breach.\n",
    "\n",
    "  %% ===================== STYLES =====================\n",
    "  classDef cfg fill:#e8f0fe,stroke:#1a73e8,stroke-width:2px,color:#0b468c;\n",
    "  classDef turn fill:#ecfeff,stroke:#06b6d4,stroke-width:2px,color:#134e4a;\n",
    "  classDef mdl fill:#fef9c3,stroke:#f59e0b,stroke-width:2px,color:#7c2d12;\n",
    "  classDef con fill:#fff7ed,stroke:#fb923c,stroke-width:2px,color:#7c2d12;\n",
    "  classDef act fill:#f5f3ff,stroke:#8b5cf6,stroke-width:2px,color:#4c1d95;\n",
    "\n",
    "  class CFG,SLO_LAT,BUD_TOK,BUD_USD,CAP_TOOL,PRICE cfg\n",
    "  class TURN,STEPS,META turn\n",
    "  class MODELS,CTOK,CLAT,CUSD mdl\n",
    "  class CON,C1,C2,C3,C4 con\n",
    "  class ACT,ENF,ADAPT,PASS,FAIL act \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a28500",
   "metadata": {},
   "source": [
    "\n",
    "# %% [SETUP]\n",
    "Miniature, **self-contained** QoS governance layer for agentic flows:\n",
    "- Tool stubs (calc/http/soap/rag)\n",
    "- A lightweight kernel (simulated LLM)\n",
    "- Parametric budgets for **latency**, **tokens**, and **cost**\n",
    "- A demo that is **not** dependent on external network calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4cd627",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [SETUP-ENV]\n",
    "import os, getpass\n",
    "os.environ.setdefault('AZURE_OPENAI_ENDPOINT', 'https://example-aoai-endpoint')\n",
    "os.environ.setdefault('AZURE_OPENAI_DEPLOYMENT', 'gpt-simulated')\n",
    "os.environ.setdefault('AZURE_OPENAI_API_VERSION', '2024-10-21')\n",
    "# Optional key input; not required for this demo.\n",
    "if not os.getenv('AZURE_OPENAI_API_KEY'):\n",
    "    try:\n",
    "        os.environ['AZURE_OPENAI_API_KEY'] = getpass.getpass('Enter AZURE_OPENAI_API_KEY (hidden, optional): ').strip()\n",
    "    except Exception:\n",
    "        os.environ['AZURE_OPENAI_API_KEY'] = 'not-needed'\n",
    "print('Environment ready (any provided key is session-only).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1681a433",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [KERNEL]\n",
    "import asyncio, time, uuid, random\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "def new_trace_id() -> str:\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "@dataclass\n",
    "class LLMResult:\n",
    "    answer: str\n",
    "    citations: List[str]\n",
    "    tokens_in: int\n",
    "    tokens_out: int\n",
    "    confidence: float\n",
    "\n",
    "class SimulatedLLM:\n",
    "    # A tiny LLM shim with token & latency estimates; no external calls.\n",
    "    def __init__(self, name=\"gpt-sim\", tps_chars=1200):\n",
    "        self.name = name\n",
    "        self.tps_chars = tps_chars  # rough chars/sec for latency simulation\n",
    "\n",
    "    async def run(self, prompt: str, grounding: Optional[List[str]] = None) -> LLMResult:\n",
    "        base = max(0.02, len(prompt) / self.tps_chars)\n",
    "        await asyncio.sleep(min(0.15, base))\n",
    "        toks_in = max(1, len(prompt)//4)\n",
    "        cites = grounding or []\n",
    "        text = f\"[{self.name}] {prompt[:120]}\"\n",
    "        if cites:\n",
    "            text += \" | cites: \" + \"; \".join(cites[:3])\n",
    "        toks_out = min(200, max(24, len(text)//4))\n",
    "        conf = 0.65 + 0.3*random.random()\n",
    "        return LLMResult(answer=text, citations=cites, tokens_in=toks_in, tokens_out=toks_out, confidence=conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6fb7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [TOOLS]\n",
    "import time\n",
    "from typing import Callable\n",
    "\n",
    "class ToolError(Exception): pass\n",
    "\n",
    "class Tool:\n",
    "    def __init__(self, name: str, fn: Callable, latency_ms: int = 80, token_cost: int = 16):\n",
    "        self.name = name\n",
    "        self.fn = fn\n",
    "        self.latency_ms = latency_ms\n",
    "        self.token_cost = token_cost  # synthetic token cost for metering\n",
    "\n",
    "    def invoke(self, **kwargs):\n",
    "        time.sleep(min(0.15, self.latency_ms/1000))\n",
    "        return self.fn(**kwargs)\n",
    "\n",
    "def tool_calc(expr: str):\n",
    "    try:\n",
    "        import re\n",
    "        if not re.fullmatch(r\"[0-9\\.\\s\\+\\-\\*\\/\\(\\)]+\", expr):\n",
    "            raise ToolError(\"Expression not allowed\")\n",
    "        return {\"expr\": expr, \"value\": eval(expr, {\"__builtins__\": {}})}\n",
    "    except Exception as e:\n",
    "        raise ToolError(str(e))\n",
    "\n",
    "def tool_http(method: str, url: str):\n",
    "    return {\"method\": method, \"url\": url, \"status\": 200, \"bytes\": 512, \"note\": \"simulated response\"}\n",
    "\n",
    "def tool_soap(action: str):\n",
    "    return {\"action\": action, \"status\": \"OK\", \"note\": \"simulated SOAP result\"}\n",
    "\n",
    "def tool_rag(query: str):\n",
    "    cites = [f\"doc://rag/{i}-{hash(query)%997}\" for i in range(1, 4)]\n",
    "    return {\"query\": query, \"citations\": cites, \"chunks\": 3}\n",
    "\n",
    "TOOLS = {\n",
    "    \"calc\": Tool(\"calc\", tool_calc, latency_ms=20, token_cost=8),\n",
    "    \"http\": Tool(\"http\", tool_http, latency_ms=60, token_cost=24),\n",
    "    \"soap\": Tool(\"soap\", tool_soap, latency_ms=90, token_cost=20),\n",
    "    \"rag\":  Tool(\"rag\",  tool_rag,  latency_ms=110, token_cost=40),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b95b6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [AGENTS]\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "@dataclass\n",
    "class QoSConfig:\n",
    "    p95_latency_target_ms: int = 2500\n",
    "    budget_tokens_per_turn: int = 4000\n",
    "    budget_usd_per_turn: float = 0.05\n",
    "    pricing_per_1k_tokens: float = 0.002\n",
    "    per_tool_caps: Dict[str, Dict[str, float]] = field(default_factory=lambda: {\n",
    "        \"calc\": {\"tokens\": 256, \"latency_ms\": 400, \"retries\": 1},\n",
    "        \"http\": {\"tokens\": 512, \"latency_ms\": 800, \"retries\": 1},\n",
    "        \"soap\": {\"tokens\": 512, \"latency_ms\": 900, \"retries\": 1},\n",
    "        \"rag\":  {\"tokens\": 1024,\"latency_ms\": 1200,\"retries\": 1},\n",
    "    })\n",
    "\n",
    "class PolicyEngine:\n",
    "    def allowlist(self, intent: str) -> List[str]:\n",
    "        return [\"calc\",\"http\",\"soap\",\"rag\"]\n",
    "    def precheck(self, prompt: str) -> bool:\n",
    "        return True\n",
    "    def postcheck(self, answer: str, citations: List[str]) -> bool:\n",
    "        return True\n",
    "\n",
    "class Planner:\n",
    "    def __init__(self, policy: PolicyEngine):\n",
    "        self.policy = policy\n",
    "    def select_tools(self, intent: str) -> List[str]:\n",
    "        allow = self.policy.allowlist(intent)\n",
    "        if intent == \"rag\": return [t for t in [\"rag\"] if t in allow]\n",
    "        if intent == \"calc\": return [t for t in [\"calc\"] if t in allow]\n",
    "        if intent == \"http\": return [t for t in [\"http\"] if t in allow]\n",
    "        if intent == \"soap\": return [t for t in [\"soap\"] if t in allow]\n",
    "        return []\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, name: str, llm: SimulatedLLM, policy: PolicyEngine, qos: QoSConfig):\n",
    "        self.name = name\n",
    "        self.llm = llm\n",
    "        self.policy = policy\n",
    "        self.qos = qos\n",
    "        self.planner = Planner(policy)\n",
    "\n",
    "    def available_tools(self) -> List[str]:\n",
    "        return list(TOOLS.keys())\n",
    "\n",
    "    def call(self, tool_name: str, **kwargs):\n",
    "        if tool_name not in TOOLS: \n",
    "            raise ToolError(f\"Unknown tool '{tool_name}'\")\n",
    "        result = TOOLS[tool_name].invoke(**kwargs)\n",
    "        return {\"tool\": tool_name, \"result\": result}\n",
    "\n",
    "    async def run(self, prompt: str, intent: Optional[str] = None) -> Dict[str, Any]:\n",
    "        trace_id = new_trace_id()\n",
    "        if not self.policy.precheck(prompt):\n",
    "            return {\"answer\": \"[refused by policy]\", \"citations\": [], \"confidence\": 0.0, \"trace_id\": trace_id}\n",
    "\n",
    "        intent = intent or \"general\"\n",
    "        chosen = self.planner.select_tools(intent)\n",
    "        grounding = []\n",
    "        if \"rag\" in chosen:\n",
    "            gr = TOOLS[\"rag\"].invoke(query=prompt)\n",
    "            grounding = gr.get(\"citations\", [])\n",
    "\n",
    "        res = await self.llm.run(prompt, grounding=grounding)\n",
    "        tokens = res.tokens_in + res.tokens_out + sum(TOOLS[t].token_cost for t in chosen)\n",
    "        cost_usd = (tokens/1000.0) * self.qos.pricing_per_1k_tokens\n",
    "\n",
    "        within_tokens = tokens <= self.qos.budget_tokens_per_turn\n",
    "        within_cost = cost_usd <= self.qos.budget_usd_per_turn\n",
    "        within_latency = True\n",
    "        pass_gate = within_tokens and within_cost and within_latency and self.policy.postcheck(res.answer, res.citations)\n",
    "\n",
    "        return {\n",
    "            \"answer\": res.answer if pass_gate else \"[degraded or refused due to QoS]\",\n",
    "            \"citations\": res.citations,\n",
    "            \"confidence\": res.confidence if pass_gate else 0.4,\n",
    "            \"tokens\": tokens,\n",
    "            \"cost_usd\": round(cost_usd, 6),\n",
    "            \"trace_id\": trace_id,\n",
    "            \"gate\": {\n",
    "                \"within_tokens\": within_tokens,\n",
    "                \"within_cost\": within_cost,\n",
    "                \"within_latency\": within_latency,\n",
    "                \"tools_used\": chosen,\n",
    "            }\n",
    "        }\n",
    "\n",
    "llm = SimulatedLLM()\n",
    "policy = PolicyEngine()\n",
    "qos = QoSConfig()\n",
    "\n",
    "agent_rag   = Agent(\"Agent-RAG\", llm, policy, qos)\n",
    "agent_calc  = Agent(\"Agent-CALC\", llm, policy, qos)\n",
    "agent_http  = Agent(\"Agent-HTTP\", llm, policy, qos)\n",
    "agent_soap  = Agent(\"Agent-SOAP\", llm, policy, qos)\n",
    "agent_general = Agent(\"Agent-GENERAL\", llm, policy, qos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b75d6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [WIRES]\n",
    "ROUTES = {\n",
    "    \"rag\": \"rag\",\n",
    "    \"calc\": \"calc\",\n",
    "    \"http\": \"http\",\n",
    "    \"soap\": \"soap\",\n",
    "    \"general\": \"general\",\n",
    "}\n",
    "AGENT_INDEX = {\n",
    "    \"rag\": agent_rag,\n",
    "    \"calc\": agent_calc,\n",
    "    \"http\": agent_http,\n",
    "    \"soap\": agent_soap,\n",
    "    \"general\": agent_general,\n",
    "}\n",
    "\n",
    "def validate_wiring():\n",
    "    problems = []\n",
    "    for intent, key in ROUTES.items():\n",
    "        agent = AGENT_INDEX.get(key)\n",
    "        if not agent:\n",
    "            problems.append(f\"{intent} -> missing agent key '{key}'\"); \n",
    "            continue\n",
    "        if not agent.available_tools():\n",
    "            problems.append(f\"{intent} -> {agent.name} has no available tools\")\n",
    "    return problems\n",
    "\n",
    "total_wires = len(ROUTES)\n",
    "distinct_agents = len(set(ROUTES.values()))\n",
    "unreferenced_agents = sorted(set(AGENT_INDEX.keys()) - set(ROUTES.values()))\n",
    "targets_by_agent = {}\n",
    "for intent, key in ROUTES.items():\n",
    "    targets_by_agent.setdefault(key, []).append(intent)\n",
    "\n",
    "issues = validate_wiring()\n",
    "print(f\"Wires: {total_wires} (distinct agents: {distinct_agents})\")\n",
    "for agent_key, intents in targets_by_agent.items():\n",
    "    agent_name = AGENT_INDEX[agent_key].name\n",
    "    print(f\"  - {agent_name} ← {len(intents)} intent(s): {', '.join(intents)}\")\n",
    "if unreferenced_agents: \n",
    "    print(f\"Unreferenced agents: {', '.join(unreferenced_agents)}\")\n",
    "print(\"Wiring OK\" if not issues else \"Wiring issues:\\\\n- \" + \"\\\\n- \".join(issues))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9a2f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [DEMO]\n",
    "# Notebook-safe async demo (no asyncio.run() when a loop exists).\n",
    "import asyncio, time\n",
    "\n",
    "samples = [\n",
    "    (\"rag\",   \"Ground this answer with citations about hybrid search.\"),\n",
    "    (\"calc\",  \"Please compute the sum 2+2.\"),\n",
    "    (\"http\",  \"Do an HTTP call to example.org\"),\n",
    "    (\"soap\",  \"Call a SOAP action for demo\"),\n",
    "    (\"general\",\"Just chat and cite if needed.\"),\n",
    "]\n",
    "\n",
    "async def demo_run():\n",
    "    t0 = time.time()\n",
    "    outputs = []\n",
    "    for intent, text in samples:\n",
    "        key = ROUTES[intent]\n",
    "        agent = AGENT_INDEX[key]\n",
    "        tool_result = None\n",
    "        if intent == \"calc\":\n",
    "            tool_result = agent.call(\"calc\", expr=\"6*7\")\n",
    "        elif intent == \"http\":\n",
    "            tool_result = agent.call(\"http\", method=\"GET\", url=\"https://example.org\")\n",
    "        elif intent == \"soap\":\n",
    "            tool_result = agent.call(\"soap\", action=\"Ping\")\n",
    "        elif intent == \"rag\":\n",
    "            tool_result = agent.call(\"rag\", query=text)\n",
    "\n",
    "        llm_out = await agent.run(text, intent=intent)\n",
    "        outputs.append({\n",
    "            \"intent\": intent,\n",
    "            \"agent\": agent.name,\n",
    "            \"tool_result\": tool_result,\n",
    "            \"llm_result\": llm_out[\"answer\"][:220] + (\"...\" if len(llm_out[\"answer\"])>220 else \"\"),\n",
    "            \"confidence\": llm_out[\"confidence\"],\n",
    "            \"tokens\": llm_out[\"tokens\"],\n",
    "            \"cost_usd\": llm_out[\"cost_usd\"],\n",
    "            \"pass_tokens\": llm_out[\"gate\"][\"within_tokens\"],\n",
    "            \"pass_cost\": llm_out[\"gate\"][\"within_cost\"],\n",
    "        })\n",
    "    elapsed_ms = int((time.time() - t0)*1000)\n",
    "    return {\"elapsed_ms\": elapsed_ms, \"runs\": outputs}\n",
    "\n",
    "try:\n",
    "    loop = asyncio.get_running_loop()\n",
    "    try:\n",
    "        result = await demo_run()\n",
    "    except SyntaxError:\n",
    "        import nest_asyncio; nest_asyncio.apply()\n",
    "        result = loop.run_until_complete(demo_run())\n",
    "except RuntimeError:\n",
    "    result = asyncio.run(demo_run())\n",
    "\n",
    "print(\"Elapsed (ms):\", result[\"elapsed_ms\"])\n",
    "for r in result[\"runs\"]:\n",
    "    print((\n",
    "        f\"\\\\nIntent: {r['intent']} -> Agent: {r['agent']}\"\n",
    "        f\"\\\\nTool: {r['tool_result']}\"\n",
    "        f\"\\\\nLLM:  {r['llm_result']} (conf={r['confidence']:.2f}, tokens={r['tokens']}, $={r['cost_usd']})\"\n",
    "        f\"\\\\nGates: tokens={r['pass_tokens']} cost={r['pass_cost']}\"\n",
    "    ).rstrip())\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}