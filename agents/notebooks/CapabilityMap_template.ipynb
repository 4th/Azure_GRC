{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "template_info": {
      "generated_at": "2025-11-07T14:28:13.529035Z",
      "note": "This file was auto-generated as a clean template (outputs cleared, cells tagged)."
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[TEMPLATE] â€” Review and adapt\n",
        "\n",
        "ðŸ§  Install Semantic Kernel\n",
        "\n",
        "To use Microsoftâ€™s Semantic Kernel for orchestrating AI agents, planners, and connectors, install the latest version from PyPI using pip.\n",
        "Run the following command in a code cell below:"
      ],
      "metadata": {
        "id": "fCuV_fkbbAGb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Og6hRczKbAGe"
      },
      "outputs": [],
      "source": [
        "# TEMPLATE: review/adapt this cell for your project\n",
        "pip install -U semantic-kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[TEMPLATE] â€” Review and adapt\n",
        "\n",
        "ðŸ’¡ Note: Removing pydrive2\n",
        "\n",
        "If you see dependency warnings about cryptography or pyOpenSSL when installing Azure packages, they usually come from pydrive2, which Colab preinstalls for legacy Google Drive APIs.\n",
        "You donâ€™t actually need pydrive2 for Drive access anymore â€” Colabâ€™s built-in drive.mount() handles everything.\n",
        "\n",
        "To remove it safely and avoid version conflicts, run:\n"
      ],
      "metadata": {
        "id": "1ujaG45sbAGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TEMPLATE: review/adapt this cell for your project\n",
        "!pip -q uninstall -y pydrive2"
      ],
      "metadata": {
        "id": "8JkwTLi-bAGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[TEMPLATE] â€” Review and adapt\n",
        "\n",
        "ðŸ¤– Connect to Azure OpenAI in Colab\n",
        "\n",
        "The following code shows how to connect Google Colab to your Azure OpenAI resource using the official openai Python package.\n",
        "\n",
        "Import the AzureOpenAI client class.\n",
        "\n",
        "Initialize it with your Azure OpenAI endpoint, API key, and API version.\n",
        "\n",
        "Use your deployment name (for example, gpt-35-turbo) as the model parameter when sending chat requests.\n",
        "\n",
        "Print the assistantâ€™s response.\n",
        "\n",
        "âš ï¸ Security Note: Never share your real API key in public notebooks or commits.\n",
        "Instead, store it in an environment variable (for example,\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"]) and reference it securely in your code.\n",
        "\n",
        "Run the next code cell to send a test message to your Azure OpenAI deployment and confirm the connection from Colab.\n",
        "\n"
      ],
      "metadata": {
        "id": "_IhEUt2FbAGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TEMPLATE: review/adapt this cell for your project\n",
        "from openai import AzureOpenAI\n",
        "\n",
        "# Initialize client\n",
        "client = AzureOpenAI(\n",
        "    api_key=\"INSERT_KEY\",\n",
        "    azure_endpoint=\"INSERT_ENDPOINT}\",\n",
        "    api_version=\"2024-10-21\"\n",
        ")\n",
        "\n",
        "# Correct model name = deployment name (not 'gpt-4' etc.)\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-35-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Hello from Colab and Azure OpenAI!\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n"
      ],
      "metadata": {
        "id": "72ojAfiEbAGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[TEMPLATE] â€” Review and adapt\n",
        "\n",
        "ðŸ§  Installing Microsoft Agentic Framework Dependencies\n",
        "\n",
        "Before running Azure OpenAI or the Microsoft Agent Framework in Colab, youâ€™ll need to install the latest compatible SDKs and libraries.\n",
        "This command ensures that Semantic Kernel, Azure AI Agents, and OpenAI are all properly aligned to avoid version conflicts.\n",
        "\n",
        "This installs:\n",
        "\n",
        "ðŸ§© semantic-kernel â€“ orchestrates AI workflows and prompt pipelines\n",
        "\n",
        "ðŸ” azure-identity â€“ handles secure authentication with Azure services\n",
        "\n",
        "â˜ï¸ azure-ai-projects â€“ project management and AI service integration\n",
        "\n",
        "ðŸ¤– azure-ai-agents â€“ enables creation of tool-using and multi-agent systems\n",
        "\n",
        "ðŸ’¬ openai â€“ communicates with Azure OpenAI endpoints for completions and chat\n",
        "\n",
        "âœ… Tip: Run this cell once per Colab session before importing any Azure or Semantic Kernel packages."
      ],
      "metadata": {
        "id": "HPaXpHsFbAGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TEMPLATE: review/adapt this cell for your project\n",
        "!pip -q install -U \"semantic-kernel==1.37.1\" \"azure-identity>=1.25.1\" \"azure-ai-projects==1.1.0b4\" \"azure-ai-agents==1.2.0b5\" \"openai>=1.98\"\n"
      ],
      "metadata": {
        "id": "eH_xvmVtbAGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[TEMPLATE] â€” Review and adapt\n",
        "\n",
        "ðŸ“‚ Work with Google Drive in Colab\n",
        "\n",
        "Use this cell to mount your Google Drive, inspect files, create a project folder, and read/write a CSV. It avoids pydrive2 by using Colabâ€™s native mount."
      ],
      "metadata": {
        "id": "6Wfe78IubAGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TEMPLATE: review/adapt this cell for your project\n",
        "# 1) Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)   # follow the auth link\n",
        "\n",
        "# 2) Your files live under /content/drive/MyDrive\n",
        "import os, pathlib, pandas as pd\n",
        "\n",
        "base = pathlib.Path(\"/content/drive/MyDrive\")\n",
        "print(\"Root exists?\", base.exists())\n",
        "print(\"Sample listing:\", os.listdir(base)[:10])\n",
        "\n",
        "# 3) Create a folder and write a file\n",
        "proj = base / \"azure_agentic_demo\"\n",
        "proj.mkdir(exist_ok=True)\n",
        "(proj / \"hello.txt\").write_text(\"Hello from Colab!\")\n",
        "\n",
        "# 4) Save & read a CSV\n",
        "df = pd.DataFrame({\"a\":[1,2,3], \"b\":[\"x\",\"y\",\"z\"]})\n",
        "csv_path = proj / \"sample.csv\"\n",
        "df.to_csv(csv_path, index=False)\n",
        "print(\"Saved:\", csv_path)\n",
        "\n",
        "pd.read_csv(csv_path).head()\n"
      ],
      "metadata": {
        "id": "gAUwbB9WbAGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[TEMPLATE] â€” Review and adapt\n",
        "\n",
        "ðŸ”‘ Configure Azure OpenAI Environment Variables\n",
        "\n",
        "Set up your Azure credentials so Colab can authenticate directly with your Azure OpenAI resource.\n",
        "Replace the sample API key and endpoint below with your own values from the Azure Portal (Keys and Endpoint section)."
      ],
      "metadata": {
        "id": "bF0h6sS3bAGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TEMPLATE: review/adapt this cell for your project\n",
        "import os\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"]   = \"https://4th-openai-resource.openai.azure.com\"\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"]    = \"XXXXX\"   # from Keys & Endpoints\n",
        "os.environ[\"AZURE_OPENAI_API_VERSION\"]= \"2024-10-21\"\n",
        "os.environ[\"AZURE_OPENAI_DEPLOYMENT\"] = \"gpt-35-turbo\"    # your deployment name\n"
      ],
      "metadata": {
        "id": "IFaXUAj6bAGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[TEMPLATE] â€” Review and adapt\n",
        "\n",
        "âœ… Quick sanity check: call your Azure OpenAI deployment\n",
        "\n",
        "Run this cell to verify your environment variables and deployment are working. It should print a short greeting; if anythingâ€™s misconfigured, youâ€™ll see a clear error."
      ],
      "metadata": {
        "id": "JKDiqWwwbAGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TEMPLATE: review/adapt this cell for your project\n",
        "from openai import AzureOpenAI, OpenAIError\n",
        "client = AzureOpenAI(\n",
        "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
        "    api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
        "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
        ")\n",
        "\n",
        "try:\n",
        "    r = client.chat.completions.create(\n",
        "        model=os.environ[\"AZURE_OPENAI_DEPLOYMENT\"],\n",
        "        messages=[{\"role\":\"user\",\"content\":\"Say hi in one short line.\"}]\n",
        "    )\n",
        "    print(r.choices[0].message.content)\n",
        "except OpenAIError as e:\n",
        "    print(\"ERROR:\", e)"
      ],
      "metadata": {
        "id": "muSktowfbAGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[TEMPLATE] â€” Review and adapt\n",
        "\n",
        "ðŸ§ª Tool-calling demo: simple agent with a timezone tool\n",
        "\n",
        "This cell shows a minimal agent loop using Azure OpenAI tool-calling. The model decides when to call the get_timezone tool, you run the tool in Python, then feed the result back for a final answer."
      ],
      "metadata": {
        "id": "d-B5aW2NbAGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TEMPLATE: review/adapt this cell for your project\n",
        "import json, datetime as dt\n",
        "\n",
        "def get_timezone(city:str)->str:\n",
        "    # stub tool; replace with real logic as needed\n",
        "    return f\"Timezone for {city}: US/Eastern (stub) @ {dt.datetime.now().isoformat(timespec='seconds')}\"\n",
        "\n",
        "tools = [{\n",
        "    \"type\":\"function\",\n",
        "    \"function\":{\n",
        "        \"name\":\"get_timezone\",\n",
        "        \"description\":\"Return a timezone for a US city\",\n",
        "        \"parameters\":{\"type\":\"object\",\n",
        "                      \"properties\":{\"city\":{\"type\":\"string\"}},\n",
        "                      \"required\":[\"city\"]},\n",
        "    },\n",
        "}]\n",
        "\n",
        "messages=[{\"role\":\"system\",\"content\":\"You are concise and call tools when helpful.\"},\n",
        "          {\"role\":\"user\",\"content\":\"What timezone is Tallahassee in?\"}]\n",
        "\n",
        "resp = client.chat.completions.create(\n",
        "    model=os.environ[\"AZURE_OPENAI_DEPLOYMENT\"],\n",
        "    messages=messages,\n",
        "    tools=tools,\n",
        "    tool_choice=\"auto\",\n",
        ")\n",
        "msg = resp.choices[0].message\n",
        "messages.append(msg)\n",
        "\n",
        "if msg.tool_calls:\n",
        "    for call in msg.tool_calls:\n",
        "        args = json.loads(call.function.arguments or \"{}\")\n",
        "        result = get_timezone(**args)\n",
        "        messages.append({\n",
        "            \"role\":\"tool\",\n",
        "            \"tool_call_id\":call.id,\n",
        "            \"name\":call.function.name,\n",
        "            \"content\": result\n",
        "        })\n",
        "    final = client.chat.completions.create(\n",
        "        model=os.environ[\"AZURE_OPENAI_DEPLOYMENT\"],\n",
        "        messages=messages\n",
        "    )\n",
        "    print(final.choices[0].message.content)\n",
        "else:\n",
        "    print(msg.content)\n"
      ],
      "metadata": {
        "id": "NpSENkeQbAGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[TEMPLATE] â€” Review and adapt\n",
        "\n",
        "ðŸ’¾ Save conversation to Google Drive (normalize SDK objects â†’ JSON)\n",
        "\n",
        "This cell converts any SDK message objects (like ChatCompletionMessage) into plain dicts so theyâ€™re JSON-serializable, then writes the transcript to Drive."
      ],
      "metadata": {
        "id": "gvxl10WQbAGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TEMPLATE: review/adapt this cell for your project\n",
        "# Normalize OpenAI message objects -> plain dicts, then save to Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import pathlib, json\n",
        "\n",
        "def normalize_messages(msgs):\n",
        "    out = []\n",
        "    for m in msgs:\n",
        "        if isinstance(m, dict):\n",
        "            out.append(m)\n",
        "            continue\n",
        "        item = {\"role\": getattr(m, \"role\", None), \"content\": getattr(m, \"content\", None)}\n",
        "        # include tool calls if present\n",
        "        tcs = getattr(m, \"tool_calls\", None)\n",
        "        if tcs:\n",
        "            item[\"tool_calls\"] = [\n",
        "                {\n",
        "                    \"id\": tc.id,\n",
        "                    \"type\": \"function\",\n",
        "                    \"function\": {\n",
        "                        \"name\": getattr(tc.function, \"name\", None),\n",
        "                        \"arguments\": getattr(tc.function, \"arguments\", None),\n",
        "                    },\n",
        "                }\n",
        "                for tc in tcs\n",
        "            ]\n",
        "        # include name if present (e.g., tool messages)\n",
        "        name = getattr(m, \"name\", None)\n",
        "        if name: item[\"name\"] = name\n",
        "        out.append(item)\n",
        "    return out\n",
        "\n",
        "log_dir = pathlib.Path(\"/content/drive/MyDrive/azure_agentic_demo\")\n",
        "log_dir.mkdir(exist_ok=True)\n",
        "normalized = normalize_messages(messages)\n",
        "path = log_dir / \"conversation.json\"\n",
        "path.write_text(json.dumps(normalized, indent=2))\n",
        "print(f\"Saved {path}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "DIpqkj4mbAGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[TEMPLATE] â€” Review and adapt\n",
        "\n",
        "âš™ï¸ Install Microsoft Agent Framework (Preview)\n",
        "\n",
        "Run this cell to install the Microsoft Agent Framework (pre-release version) into your Colab environment:"
      ],
      "metadata": {
        "id": "PWy2OaBmbAGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TEMPLATE: review/adapt this cell for your project\n",
        "pip install agent-framework --pre"
      ],
      "metadata": {
        "id": "vwFXXhwVbAGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[TEMPLATE] â€” Review and adapt\n",
        "\n"
      ],
      "metadata": {
        "id": "B3EYmZt8bAGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TEMPLATE: review/adapt this cell for your project\n",
        "# === RAG over Drive files (PDF/DOCX/TXT/MD) => embed (local) => retrieve => answer with Azure OpenAI ===\n",
        "# Turnkey single cell for Google Colab\n",
        "\n",
        "# --- Install deps (idempotent) ---\n",
        "import sys, subprocess, importlib\n",
        "def pip_install(pkgs):\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-U\", *pkgs], check=True)\n",
        "\n",
        "for mod, pkgs in {\n",
        "    \"faiss\": [\"faiss-cpu\"],\n",
        "    \"sentence_transformers\": [\"sentence-transformers\"],\n",
        "    \"pypdf\": [\"pypdf\"],\n",
        "    \"docx2txt\": [\"docx2txt\"],\n",
        "}.items():\n",
        "    try:\n",
        "        importlib.import_module(mod)\n",
        "    except Exception:\n",
        "        pip_install(pkgs)\n",
        "\n",
        "# --- Imports ---\n",
        "from google.colab import drive\n",
        "from pypdf import PdfReader\n",
        "import docx2txt\n",
        "import os, pathlib, re, json, hashlib, pickle\n",
        "import numpy as np, faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from openai import AzureOpenAI\n",
        "\n",
        "# --- Mount Drive ---\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "# ========================= CONFIG =========================\n",
        "# Put your docs into this folder in Drive:\n",
        "DATA_DIR = pathlib.Path(\"/content/drive/MyDrive\")\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Where to save FAISS and metadata:\n",
        "INDEX_DIR = pathlib.Path(\"/content/drive/MyDrive/RAG_index\")\n",
        "INDEX_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Ask your question here:\n",
        "QUESTION = \"Summarize the key policies and dates from the syllabus.\"\n",
        "TOP_K = 5\n",
        "# =========================================================\n",
        "\n",
        "print(\"Place files in:\", DATA_DIR)\n",
        "print(\"Found files:\", [p.name for p in DATA_DIR.iterdir() if p.suffix.lower() in [\".pdf\",\".docx\",\".txt\",\".md\"]])\n",
        "\n",
        "# --- Utilities: load & chunk ---\n",
        "def read_file(path: pathlib.Path) -> str:\n",
        "    suf = path.suffix.lower()\n",
        "    if suf == \".pdf\":\n",
        "        text = []\n",
        "        with open(path, \"rb\") as f:\n",
        "            reader = PdfReader(f)\n",
        "            for pg in reader.pages:\n",
        "                text.append(pg.extract_text() or \"\")\n",
        "        return \"\\n\".join(text)\n",
        "    if suf == \".docx\":\n",
        "        return docx2txt.process(str(path)) or \"\"\n",
        "    if suf in [\".txt\", \".md\"]:\n",
        "        return path.read_text(errors=\"ignore\")\n",
        "    return \"\"\n",
        "\n",
        "def clean_text(s: str) -> str:\n",
        "    s = s.replace(\"\\x00\", \" \")\n",
        "    s = re.sub(r\"\\s+\\n\", \"\\n\", s)\n",
        "    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s)\n",
        "    return s.strip()\n",
        "\n",
        "def chunk_text(text: str, max_words=300, overlap=60):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    i = 0\n",
        "    while i < len(words):\n",
        "        chunk = words[i:i+max_words]\n",
        "        if not chunk: break\n",
        "        chunks.append(\" \".join(chunk))\n",
        "        i += max_words - overlap\n",
        "    return chunks\n",
        "\n",
        "def file_digest(path: pathlib.Path):\n",
        "    h = hashlib.sha256()\n",
        "    with open(path, \"rb\") as f:\n",
        "        for b in iter(lambda: f.read(1 << 20), b\"\"):\n",
        "            h.update(b)\n",
        "    return h.hexdigest()\n",
        "\n",
        "# --- Gather docs -> chunks + metadata ---\n",
        "docs = []\n",
        "file_hashes = {}\n",
        "for p in sorted(DATA_DIR.iterdir()):\n",
        "    if p.suffix.lower() not in [\".pdf\", \".docx\", \".txt\", \".md\"]:\n",
        "        continue\n",
        "    raw = read_file(p)\n",
        "    txt = clean_text(raw)\n",
        "    if not txt.strip():\n",
        "        continue\n",
        "    fh = file_digest(p)\n",
        "    file_hashes[p.name] = fh\n",
        "    for j, ch in enumerate(chunk_text(txt, max_words=300, overlap=60)):\n",
        "        docs.append({\"doc\": p.name, \"digest\": fh, \"chunk_id\": j, \"text\": ch})\n",
        "\n",
        "if not docs:\n",
        "    raise SystemExit(\"No supported documents found. Add PDF/DOCX/TXT/MD files to /MyDrive/RAG_docs and rerun.\")\n",
        "\n",
        "print(f\"Prepared {len(docs)} chunks from {len(file_hashes)} files.\")\n",
        "\n",
        "# --- Build or load embeddings + FAISS ---\n",
        "embed_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embed_model = SentenceTransformer(embed_model_name)\n",
        "\n",
        "index_path = INDEX_DIR / \"index.faiss\"\n",
        "meta_path  = INDEX_DIR / \"meta.pkl\"\n",
        "hash_path  = INDEX_DIR / \"file_hashes.json\"\n",
        "\n",
        "rebuild = True\n",
        "if index_path.exists() and meta_path.exists() and hash_path.exists():\n",
        "    try:\n",
        "        old_hashes = json.loads(hash_path.read_text())\n",
        "        if old_hashes == file_hashes:\n",
        "            # Load existing index/meta\n",
        "            index = faiss.read_index(str(index_path))\n",
        "            with open(meta_path, \"rb\") as f:\n",
        "                meta_docs = pickle.load(f)\n",
        "            # Sanity: meta_docs should match docs length; if not, rebuild\n",
        "            if len(meta_docs) == len(docs):\n",
        "                docs = meta_docs\n",
        "                rebuild = False\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "if rebuild:\n",
        "    print(\"Embedding and indexingâ€¦\")\n",
        "    embs = embed_model.encode(\n",
        "        [d[\"text\"] for d in docs],\n",
        "        batch_size=64,\n",
        "        show_progress_bar=True,\n",
        "        normalize_embeddings=True\n",
        "    )\n",
        "    embs = np.asarray(embs, dtype=\"float32\")\n",
        "    index = faiss.IndexFlatIP(embs.shape[1])  # cosine (with normalized vectors)\n",
        "    index.add(embs)\n",
        "    faiss.write_index(index, str(index_path))\n",
        "    with open(meta_path, \"wb\") as f:\n",
        "        pickle.dump(docs, f)\n",
        "    hash_path.write_text(json.dumps(file_hashes, indent=2))\n",
        "    print(f\"Indexed {len(docs)} chunks.\")\n",
        "else:\n",
        "    print(\"Loaded existing index.\")\n",
        "\n",
        "# --- Retriever ---\n",
        "def retrieve(query: str, k=TOP_K):\n",
        "    qvec = embed_model.encode([query], normalize_embeddings=True)\n",
        "    D, I = index.search(np.asarray(qvec, dtype=\"float32\"), k)\n",
        "    results = []\n",
        "    for rank, i in enumerate(I[0]):\n",
        "        if i == -1: continue\n",
        "        m = docs[i]\n",
        "        results.append({\n",
        "            \"rank\": rank + 1,\n",
        "            \"score\": float(D[0][rank]),\n",
        "            \"doc\": m[\"doc\"],\n",
        "            \"chunk_id\": m[\"chunk_id\"],\n",
        "            \"text\": m[\"text\"]\n",
        "        })\n",
        "    return results\n",
        "\n",
        "def make_context(snippets):\n",
        "    blocks = []\n",
        "    for s in snippets:\n",
        "        header = f\"[{s['doc']} #chunk:{s['chunk_id']} score:{s['score']:.3f}]\"\n",
        "        blocks.append(f\"{header}\\n{s['text']}\")\n",
        "    return \"\\n\\n---\\n\\n\".join(blocks)\n",
        "\n",
        "# --- Azure OpenAI client (expects env vars already set) ---\n",
        "missing = [k for k in [\"AZURE_OPENAI_ENDPOINT\",\"AZURE_OPENAI_API_KEY\",\"AZURE_OPENAI_API_VERSION\",\"AZURE_OPENAI_DEPLOYMENT\"] if not os.getenv(k)]\n",
        "if missing:\n",
        "    raise SystemExit(f\"Missing env vars: {missing}. Set them before running this cell.\")\n",
        "\n",
        "client = AzureOpenAI(\n",
        "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
        "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
        "    api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
        ")\n",
        "\n",
        "def answer_with_rag(question: str, top_k: int = TOP_K):\n",
        "    hits = retrieve(question, k=top_k)\n",
        "    context = make_context(hits)\n",
        "    prompt = (\n",
        "        \"Use the CONTEXT to answer the QUESTION concisely. \"\n",
        "        \"If the answer is not in the context, say you don't know.\\n\\n\"\n",
        "        f\"QUESTION:\\n{question}\\n\\n\"\n",
        "        f\"CONTEXT:\\n{context}\\n\\n\"\n",
        "        \"Return citations like [doc #chunk:X].\"\n",
        "    )\n",
        "    resp = client.chat.completions.create(\n",
        "        model=os.environ[\"AZURE_OPENAI_DEPLOYMENT\"],  # deployment name, e.g., gpt-35-turbo\n",
        "        messages=[\n",
        "            {\"role\":\"system\",\"content\":\"You are a helpful assistant. Always cite sources as [doc #chunk:X].\"},\n",
        "            {\"role\":\"user\",\"content\": prompt}\n",
        "        ],\n",
        "        temperature=0.2,\n",
        "    )\n",
        "    return resp.choices[0].message.content, hits\n",
        "\n",
        "# --- Ask the question ---\n",
        "print(\"\\n=== QUESTION ===\")\n",
        "print(QUESTION)\n",
        "answer, sources = answer_with_rag(QUESTION, top_k=TOP_K)\n",
        "\n",
        "print(\"\\n=== ANSWER ===\")\n",
        "print(answer)\n",
        "\n",
        "print(\"\\n=== SOURCES ===\")\n",
        "for s in sources:\n",
        "    print(f\"{s['rank']:>2}. {s['doc']}  #chunk:{s['chunk_id']}  score:{s['score']:.3f}\")\n"
      ],
      "metadata": {
        "id": "K7ByWnhfbAGl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}