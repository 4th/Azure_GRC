{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e5fce6e",
   "metadata": {
    "tags": [
     "TITLE"
    ]
   },
   "source": [
    "# Assembled Notebook — SK vs LangGraph Decision Agents\n",
    "_Generated 2025-11-07T23:58:08.471563Z_\n",
    "\n",
    "> Agents and tools are derived from your decision tree. The diagram itself is **not** embedded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d2bccb",
   "metadata": {
    "tags": [
     "SETUP"
    ]
   },
   "outputs": [],
   "source": [
    "# %% [SETUP]\n",
    "!pip install -U semantic-kernel\n",
    "!pip -q uninstall -y pydrive2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30db0fa1",
   "metadata": {
    "tags": [
     "SETUP-ENV"
    ]
   },
   "outputs": [],
   "source": [
    "# %% [SETUP-ENV]\n",
    "import os, getpass\n",
    "os.environ.setdefault('AZURE_OPENAI_ENDPOINT', 'https://4th-openai-resource.openai.azure.com')\n",
    "os.environ.setdefault('AZURE_OPENAI_DEPLOYMENT', 'gpt-35-turbo')\n",
    "os.environ.setdefault('AZURE_OPENAI_API_VERSION', '2024-10-21')\n",
    "if not os.getenv('AZURE_OPENAI_API_KEY'):\n",
    "    os.environ['AZURE_OPENAI_API_KEY'] = getpass.getpass('Enter AZURE_OPENAI_API_KEY (hidden): ').strip()\n",
    "print('Azure OpenAI env ready (key is session-only).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd49ed6",
   "metadata": {
    "tags": [
     "KERNEL"
    ]
   },
   "outputs": [],
   "source": [
    "# %% [KERNEL]\n",
    "import os\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "kernel = Kernel()\n",
    "\n",
    "service = AzureChatCompletion(\n",
    "    service_id='azure',\n",
    "    api_key=os.getenv('AZURE_OPENAI_API_KEY'),\n",
    "    deployment_name=os.getenv('AZURE_OPENAI_DEPLOYMENT'),\n",
    "    endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),\n",
    ")\n",
    "kernel.add_service(service)\n",
    "print('Kernel ready (Azure OpenAI)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae84f1a0",
   "metadata": {
    "tags": [
     "TOOLS"
    ]
   },
   "outputs": [],
   "source": [
    "# %% [TOOLS]\n",
    "\n",
    "def tool_assess_ecosystem(**kwargs):\n",
    "    \"\"\"Assess the primary ecosystem (Microsoft-first vs Python-first).\"\"\"\n",
    "    return \"stub:assess_ecosystem \" + str(kwargs)\n",
    "\n",
    "def tool_check_compliance_net(**kwargs):\n",
    "    \"\"\"Evaluate compliance/networking posture (PE, AAD/MSI, KV RBAC vs multi-cloud).\"\"\"\n",
    "    return \"stub:check_compliance_net \" + str(kwargs)\n",
    "\n",
    "def tool_score_integrations(**kwargs):\n",
    "    \"\"\"Score required native integrations (PVA/Teams/Graph/Fabric vs custom/web/RDBMS).\"\"\"\n",
    "    return \"stub:score_integrations \" + str(kwargs)\n",
    "\n",
    "def tool_rate_orchestration(**kwargs):\n",
    "    \"\"\"Rate orchestration complexity (simple planner vs branching graphs/crews/state).\"\"\"\n",
    "    return \"stub:rate_orchestration \" + str(kwargs)\n",
    "\n",
    "def tool_profile_team_ops(**kwargs):\n",
    "    \"\"\"Profile team skills and delivery ops (.NET/AzDO vs Python/LangChain).\"\"\"\n",
    "    return \"stub:profile_team_ops \" + str(kwargs)\n",
    "\n",
    "def tool_posture_latency_cost(**kwargs):\n",
    "    \"\"\"Assess latency/cost posture (SLO/APIM/AOAI quotas vs batching/local models).\"\"\"\n",
    "    return \"stub:posture_latency_cost \" + str(kwargs)\n",
    "\n",
    "def tool_hybrid_candidate(**kwargs):\n",
    "    \"\"\"Determine if a hybrid SK↔LangGraph approach is warranted.\"\"\"\n",
    "    return \"stub:hybrid_candidate \" + str(kwargs)\n",
    "\n",
    "def tool_trace_log(**kwargs):\n",
    "    \"\"\"Append telemetry (stage, inputs, scores).\"\"\"\n",
    "    return \"stub:trace_log \" + str(kwargs)\n",
    "\n",
    "\n",
    "TOOLS = {\n",
    "\n",
    "    'tool_assess_ecosystem': tool_assess_ecosystem,\n",
    "\n",
    "    'tool_check_compliance_net': tool_check_compliance_net,\n",
    "\n",
    "    'tool_score_integrations': tool_score_integrations,\n",
    "\n",
    "    'tool_rate_orchestration': tool_rate_orchestration,\n",
    "\n",
    "    'tool_profile_team_ops': tool_profile_team_ops,\n",
    "\n",
    "    'tool_posture_latency_cost': tool_posture_latency_cost,\n",
    "\n",
    "    'tool_hybrid_candidate': tool_hybrid_candidate,\n",
    "\n",
    "    'tool_trace_log': tool_trace_log,\n",
    "\n",
    "}\n",
    "print('Tools:', list(TOOLS.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66de431",
   "metadata": {
    "tags": [
     "AGENTS"
    ]
   },
   "outputs": [],
   "source": [
    "# %% [AGENTS]\n",
    "\n",
    "class Agent_ecosystem_recommender:\n",
    "    def __init__(self, kernel):\n",
    "        self.kernel = kernel\n",
    "        self.name = \"Ecosystem Recommender\"\n",
    "        self.system_message = \"Decide SK vs LangGraph based on primary ecosystem.\"\n",
    "        self.skills = [\"tool_assess_ecosystem\", \"tool_trace_log\"]\n",
    "    async def run(self, user_text: str) -> str:\n",
    "        try:\n",
    "            result = await self.kernel.invoke_prompt(self.system_message + \"\\n\\nUser: \" + user_text)\n",
    "            return str(result)\n",
    "        except Exception as e:\n",
    "            return f\"[Ecosystem Recommender stub] Adjust SK call. Error: {e}\"\n",
    "    def available_tools(self):\n",
    "        return [t for t in self.skills if t in TOOLS]\n",
    "    def call(self, tool_name: str, **kwargs):\n",
    "        fn = TOOLS.get(tool_name)\n",
    "        if not fn:\n",
    "            raise ValueError(f\"Tool not found: {tool_name}\")\n",
    "        return fn(**kwargs)\n",
    "\n",
    "class Agent_compliance_evaluator:\n",
    "    def __init__(self, kernel):\n",
    "        self.kernel = kernel\n",
    "        self.name = \"Compliance/Networking Evaluator\"\n",
    "        self.system_message = \"Assess compliance & networking fit (Azure PE/AAD/KV vs multi-cloud).\"\n",
    "        self.skills = [\"tool_check_compliance_net\", \"tool_trace_log\"]\n",
    "    async def run(self, user_text: str) -> str:\n",
    "        try:\n",
    "            result = await self.kernel.invoke_prompt(self.system_message + \"\\n\\nUser: \" + user_text)\n",
    "            return str(result)\n",
    "        except Exception as e:\n",
    "            return f\"[Compliance/Networking Evaluator stub] Adjust SK call. Error: {e}\"\n",
    "    def available_tools(self):\n",
    "        return [t for t in self.skills if t in TOOLS]\n",
    "    def call(self, tool_name: str, **kwargs):\n",
    "        fn = TOOLS.get(tool_name)\n",
    "        if not fn:\n",
    "            raise ValueError(f\"Tool not found: {tool_name}\")\n",
    "        return fn(**kwargs)\n",
    "\n",
    "class Agent_integration_analyzer:\n",
    "    def __init__(self, kernel):\n",
    "        self.kernel = kernel\n",
    "        self.name = \"Integration Analyzer\"\n",
    "        self.system_message = \"Map integration needs to SK native vs custom stack strengths.\"\n",
    "        self.skills = [\"tool_score_integrations\", \"tool_trace_log\"]\n",
    "    async def run(self, user_text: str) -> str:\n",
    "        try:\n",
    "            result = await self.kernel.invoke_prompt(self.system_message + \"\\n\\nUser: \" + user_text)\n",
    "            return str(result)\n",
    "        except Exception as e:\n",
    "            return f\"[Integration Analyzer stub] Adjust SK call. Error: {e}\"\n",
    "    def available_tools(self):\n",
    "        return [t for t in self.skills if t in TOOLS]\n",
    "    def call(self, tool_name: str, **kwargs):\n",
    "        fn = TOOLS.get(tool_name)\n",
    "        if not fn:\n",
    "            raise ValueError(f\"Tool not found: {tool_name}\")\n",
    "        return fn(**kwargs)\n",
    "\n",
    "class Agent_orchestration_evaluator:\n",
    "    def __init__(self, kernel):\n",
    "        self.kernel = kernel\n",
    "        self.name = \"Orchestration Evaluator\"\n",
    "        self.system_message = \"Judge complexity (planner vs branching graphs/crews/state).\"\n",
    "        self.skills = [\"tool_rate_orchestration\", \"tool_trace_log\"]\n",
    "    async def run(self, user_text: str) -> str:\n",
    "        try:\n",
    "            result = await self.kernel.invoke_prompt(self.system_message + \"\\n\\nUser: \" + user_text)\n",
    "            return str(result)\n",
    "        except Exception as e:\n",
    "            return f\"[Orchestration Evaluator stub] Adjust SK call. Error: {e}\"\n",
    "    def available_tools(self):\n",
    "        return [t for t in self.skills if t in TOOLS]\n",
    "    def call(self, tool_name: str, **kwargs):\n",
    "        fn = TOOLS.get(tool_name)\n",
    "        if not fn:\n",
    "            raise ValueError(f\"Tool not found: {tool_name}\")\n",
    "        return fn(**kwargs)\n",
    "\n",
    "class Agent_teamops_evaluator:\n",
    "    def __init__(self, kernel):\n",
    "        self.kernel = kernel\n",
    "        self.name = \"Team Skills & Ops Evaluator\"\n",
    "        self.system_message = \"Evaluate team skills and pipelines (.NET/AzDO vs Python/mkdocs/pytest).\"\n",
    "        self.skills = [\"tool_profile_team_ops\", \"tool_trace_log\"]\n",
    "    async def run(self, user_text: str) -> str:\n",
    "        try:\n",
    "            result = await self.kernel.invoke_prompt(self.system_message + \"\\n\\nUser: \" + user_text)\n",
    "            return str(result)\n",
    "        except Exception as e:\n",
    "            return f\"[Team Skills & Ops Evaluator stub] Adjust SK call. Error: {e}\"\n",
    "    def available_tools(self):\n",
    "        return [t for t in self.skills if t in TOOLS]\n",
    "    def call(self, tool_name: str, **kwargs):\n",
    "        fn = TOOLS.get(tool_name)\n",
    "        if not fn:\n",
    "            raise ValueError(f\"Tool not found: {tool_name}\")\n",
    "        return fn(**kwargs)\n",
    "\n",
    "class Agent_latencycost_evaluator:\n",
    "    def __init__(self, kernel):\n",
    "        self.kernel = kernel\n",
    "        self.name = \"Latency & Cost Evaluator\"\n",
    "        self.system_message = \"Assess SLO/quotas/caching vs batching/local model needs.\"\n",
    "        self.skills = [\"tool_posture_latency_cost\", \"tool_trace_log\"]\n",
    "    async def run(self, user_text: str) -> str:\n",
    "        try:\n",
    "            result = await self.kernel.invoke_prompt(self.system_message + \"\\n\\nUser: \" + user_text)\n",
    "            return str(result)\n",
    "        except Exception as e:\n",
    "            return f\"[Latency & Cost Evaluator stub] Adjust SK call. Error: {e}\"\n",
    "    def available_tools(self):\n",
    "        return [t for t in self.skills if t in TOOLS]\n",
    "    def call(self, tool_name: str, **kwargs):\n",
    "        fn = TOOLS.get(tool_name)\n",
    "        if not fn:\n",
    "            raise ValueError(f\"Tool not found: {tool_name}\")\n",
    "        return fn(**kwargs)\n",
    "\n",
    "class Agent_hybrid_coordinator:\n",
    "    def __init__(self, kernel):\n",
    "        self.kernel = kernel\n",
    "        self.name = \"Hybrid Coordinator\"\n",
    "        self.system_message = \"If both strengths are needed, propose SK\\u2192LangGraph via /run-graph.\"\n",
    "        self.skills = [\"tool_hybrid_candidate\", \"tool_trace_log\"]\n",
    "    async def run(self, user_text: str) -> str:\n",
    "        try:\n",
    "            result = await self.kernel.invoke_prompt(self.system_message + \"\\n\\nUser: \" + user_text)\n",
    "            return str(result)\n",
    "        except Exception as e:\n",
    "            return f\"[Hybrid Coordinator stub] Adjust SK call. Error: {e}\"\n",
    "    def available_tools(self):\n",
    "        return [t for t in self.skills if t in TOOLS]\n",
    "    def call(self, tool_name: str, **kwargs):\n",
    "        fn = TOOLS.get(tool_name)\n",
    "        if not fn:\n",
    "            raise ValueError(f\"Tool not found: {tool_name}\")\n",
    "        return fn(**kwargs)\n",
    "\n",
    "\n",
    "# Instances\n",
    "\n",
    "agent_ecosystem_recommender = Agent_ecosystem_recommender(kernel)\n",
    "\n",
    "agent_compliance_evaluator = Agent_compliance_evaluator(kernel)\n",
    "\n",
    "agent_integration_analyzer = Agent_integration_analyzer(kernel)\n",
    "\n",
    "agent_orchestration_evaluator = Agent_orchestration_evaluator(kernel)\n",
    "\n",
    "agent_teamops_evaluator = Agent_teamops_evaluator(kernel)\n",
    "\n",
    "agent_latencycost_evaluator = Agent_latencycost_evaluator(kernel)\n",
    "\n",
    "agent_hybrid_coordinator = Agent_hybrid_coordinator(kernel)\n",
    "\n",
    "print('Agents:', ['agent_ecosystem_recommender', 'agent_compliance_evaluator', 'agent_integration_analyzer', 'agent_orchestration_evaluator', 'agent_teamops_evaluator', 'agent_latencycost_evaluator', 'agent_hybrid_coordinator'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c2008d",
   "metadata": {
    "tags": [
     "WIRES"
    ]
   },
   "outputs": [],
   "source": [
    "# %% [WIRES]\n",
    "WIRES = {\n",
    "  \"Ecosystem Recommender\": {\n",
    "    \"tools\": [\n",
    "      \"tool_assess_ecosystem\",\n",
    "      \"tool_trace_log\"\n",
    "    ]\n",
    "  },\n",
    "  \"Compliance/Networking Evaluator\": {\n",
    "    \"tools\": [\n",
    "      \"tool_check_compliance_net\",\n",
    "      \"tool_trace_log\"\n",
    "    ]\n",
    "  },\n",
    "  \"Integration Analyzer\": {\n",
    "    \"tools\": [\n",
    "      \"tool_score_integrations\",\n",
    "      \"tool_trace_log\"\n",
    "    ]\n",
    "  },\n",
    "  \"Orchestration Evaluator\": {\n",
    "    \"tools\": [\n",
    "      \"tool_rate_orchestration\",\n",
    "      \"tool_trace_log\"\n",
    "    ]\n",
    "  },\n",
    "  \"Team Skills & Ops Evaluator\": {\n",
    "    \"tools\": [\n",
    "      \"tool_profile_team_ops\",\n",
    "      \"tool_trace_log\"\n",
    "    ]\n",
    "  },\n",
    "  \"Latency & Cost Evaluator\": {\n",
    "    \"tools\": [\n",
    "      \"tool_posture_latency_cost\",\n",
    "      \"tool_trace_log\"\n",
    "    ]\n",
    "  },\n",
    "  \"Hybrid Coordinator\": {\n",
    "    \"tools\": [\n",
    "      \"tool_hybrid_candidate\",\n",
    "      \"tool_trace_log\"\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "print('Wiring entries:', len(WIRES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433529a7",
   "metadata": {
    "tags": [
     "DEMO"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# %% [DEMO]\n",
    "import os, getpass, types, asyncio\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "os.environ.setdefault(\"AZURE_OPENAI_ENDPOINT\",    \"https://4th-openai-resource.openai.azure.com\")\n",
    "os.environ.setdefault(\"AZURE_OPENAI_DEPLOYMENT\",  \"gpt-35-turbo\")\n",
    "os.environ.setdefault(\"AZURE_OPENAI_API_VERSION\", \"2024-10-21\")\n",
    "if not os.getenv(\"AZURE_OPENAI_API_KEY\"):\n",
    "    os.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass(\"Enter AZURE_OPENAI_API_KEY (hidden): \").strip()\n",
    "\n",
    "try:\n",
    "    kernel\n",
    "except NameError:\n",
    "    kernel = Kernel()\n",
    "try:\n",
    "    kernel.remove_service(\"azure\")\n",
    "except Exception:\n",
    "    pass\n",
    "kernel.add_service(AzureChatCompletion(\n",
    "    service_id=\"azure\",\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    deployment_name=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "))\n",
    "\n",
    "async def _run_with_azure(self, user_text: str):\n",
    "    prompt = (getattr(self, \"system_message\", \"\") or \"\") + \"\\\\n\\\\nUser: \" + str(user_text)\n",
    "    result = await self.kernel.invoke_prompt(prompt, service_id=\"azure\")\n",
    "    return str(result)\n",
    "\n",
    "patched = []\n",
    "for name, obj in list(globals().items()):\n",
    "    if name.startswith(\"agent_\"):\n",
    "        try:\n",
    "            obj.kernel = kernel\n",
    "            obj.run = types.MethodType(_run_with_azure, obj)\n",
    "            patched.append(name)\n",
    "        except Exception:\n",
    "            pass\n",
    "print(\"Patched run() for:\", patched if patched else \"(none)\")\n",
    "\n",
    "async def demo():\n",
    "    eco = globals().get(\"agent_ecosystem_recommender\")\n",
    "    comp = globals().get(\"agent_compliance_evaluator\")\n",
    "    orch = globals().get(\"agent_orchestration_evaluator\")\n",
    "\n",
    "    print(\"Tools (ecosystem):\", eco.available_tools() if eco else [])\n",
    "    if eco and eco.available_tools():\n",
    "        print(\"assess_ecosystem:\", eco.call(\"tool_assess_ecosystem\", primary=\"Microsoft-first\"))\n",
    "    print(\"Tools (compliance):\", comp.available_tools() if comp else [])\n",
    "    if comp and comp.available_tools():\n",
    "        print(\"check_compliance_net:\", comp.call(\"tool_check_compliance_net\", azure_private_endpoints=True, aad_msi=True))\n",
    "    print(\"LLM demo:\")\n",
    "    try:\n",
    "        out = await eco.run(\"We are MS-first, with AAD/KV/PE, simple orchestration. Recommend SK or LangGraph?\")\n",
    "        print(out)\n",
    "    except Exception as e:\n",
    "        print(\"[demo] invoke failed:\", e)\n",
    "\n",
    "await demo()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
