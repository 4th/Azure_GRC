{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d03a9047",
   "metadata": {},
   "source": [
    "\n",
    "# REQ→TEST Matrix — Agents & Demo\n",
    "\n",
    "Sections: **[SETUP] · [SETUP-ENV] · [KERNEL] · [TOOLS] · [AGENTS] · [WIRES] · [DEMO]**\n",
    "\n",
    "Mermaid diagram rendered inline below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4cc233",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [SETUP] Mermaid renderer\n",
    "from IPython.display import HTML, display\n",
    "_mermaid = r\"\"\"\n",
    "flowchart LR \n",
    "  %% ===================== REQ-TO-TEST MATRIX =====================\n",
    "  %% Target outcomes ↔ metrics (accuracy, p95 latency, $/turn)\n",
    "\n",
    "  %% -------- REQUIREMENTS (TARGET OUTCOMES) --------\n",
    "  subgraph REQ[\"Requirements (Target Outcomes)\"]\n",
    "    R1[REQ-001\\n\"Grounded answers are accurate\"]\n",
    "    R2[REQ-002\\n\"Fast responses at scale\"]\n",
    "    R3[REQ-003\\n\"Cost efficiency per turn\"]\n",
    "  end\n",
    "\n",
    "  %% -------- METRICS (WHAT WE MEASURE) --------\n",
    "  subgraph MET[\"KPIs / Metrics\"]\n",
    "    M1[Accuracy@\\nFactuality ≥ 0.90\\nCitation coverage ≥ 0.85]\n",
    "    M2[Latency@\\np95 ≤ 1200 ms\\nError rate < 0.5%]\n",
    "    M3[Cost@\\n$/turn ≤ $0.012\\nTokens/turn budget]\n",
    "  end\n",
    "\n",
    "  %% -------- TESTS (HOW WE VERIFY) --------\n",
    "  subgraph TST[\"Tests / Evaluations\"]\n",
    "    T1[Eval-Fact:\\nPF/SK batch judge\\n(extractive + LLM-as-judge)]\n",
    "    T2[Eval-Cite:\\nCoverage + link validity]\n",
    "    T3[Perf-Load:\\nRPS ramp; p95, p99]\n",
    "    T4[Chaos-Retry:\\nBreaker, backoff, errors]\n",
    "    T5[Cost-Meter:\\nper-intent token caps]\n",
    "    T6[Budget-Guard:\\ncheaper-model fallback]\n",
    "  end\n",
    "\n",
    "  %% -------- EVIDENCE / REPORTING --------\n",
    "  subgraph OBS[\"Evidence & Reporting\"]\n",
    "    DSH[Dashboard:\\nweekly trendlines]\n",
    "    RUNS[(Eval run store)]\n",
    "    TRC[(Per-turn traces)]\n",
    "    GATE[CI Gate:\\nblock on regression]\n",
    "  end\n",
    "\n",
    "  %% -------- TRACEABILITY LINKS --------\n",
    "  R1 --> M1\n",
    "  R1 --> M1\n",
    "  R1 --> T1\n",
    "  R1 --> T2\n",
    "\n",
    "  R2 --> M2\n",
    "  R2 --> T3\n",
    "  R2 --> T4\n",
    "\n",
    "  R3 --> M3\n",
    "  R3 --> T5\n",
    "  R3 --> T6\n",
    "\n",
    "  %% Metrics feed tests & evidence\n",
    "  T1 --> RUNS --> DSH\n",
    "  T2 --> RUNS\n",
    "  T3 --> TRC --> DSH\n",
    "  T4 --> TRC\n",
    "  T5 --> TRC\n",
    "  T6 --> TRC\n",
    "\n",
    "  %% Release control\n",
    "  DSH --> GATE\n",
    "\n",
    "  %% ---------------- NOTES ----------------\n",
    "  %% - Update thresholds per tenant/intent in config-as-code.\n",
    "  %% - Gate deploys on: (Factuality < 0.90) OR (p95 > SLO) OR ($/turn > budget).\n",
    "\n",
    "  %% ---------------- STYLES ----------------\n",
    "  classDef req fill:#e8f0fe,stroke:#1a73e8,stroke-width:2px,color:#0b468c;\n",
    "  classDef met fill:#fef9c3,stroke:#f59e0b,stroke-width:2px,color:#7c2d12;\n",
    "  classDef tst fill:#ecfeff,stroke:#06b6d4,stroke-width:2px,color:#134e4a;\n",
    "  classDef obs fill:#f5f3ff,stroke:#8b5cf6,stroke-width:2px,color:#4c1d95;\n",
    "\n",
    "  class REQ,R1,R2,R3 req\n",
    "  class MET,M1,M2,M3 met\n",
    "  class TST,T1,T2,T3,T4,T5,T6 tst\n",
    "  class OBS,DSH,RUNS,TRC,GATE obs\n",
    "\"\"\"\n",
    "display(HTML(f\"\"\"\n",
    "<div id='mrmmd' class='mermaid' style='background:#fff;padding:12px;border:1px solid #ddd;border-radius:8px;'></div>\n",
    "<script>\n",
    "(function() {\n",
    "  const el = document.getElementById('mrmmd');\n",
    "  el.textContent = `\n",
    "flowchart LR \n",
    "  %% ===================== REQ-TO-TEST MATRIX =====================\n",
    "  %% Target outcomes ↔ metrics (accuracy, p95 latency, $/turn)\n",
    "\n",
    "  %% -------- REQUIREMENTS (TARGET OUTCOMES) --------\n",
    "  subgraph REQ[\"Requirements (Target Outcomes)\"]\n",
    "    R1[REQ-001\\n\"Grounded answers are accurate\"]\n",
    "    R2[REQ-002\\n\"Fast responses at scale\"]\n",
    "    R3[REQ-003\\n\"Cost efficiency per turn\"]\n",
    "  end\n",
    "\n",
    "  %% -------- METRICS (WHAT WE MEASURE) --------\n",
    "  subgraph MET[\"KPIs / Metrics\"]\n",
    "    M1[Accuracy@\\nFactuality ≥ 0.90\\nCitation coverage ≥ 0.85]\n",
    "    M2[Latency@\\np95 ≤ 1200 ms\\nError rate < 0.5%]\n",
    "    M3[Cost@\\n$/turn ≤ $0.012\\nTokens/turn budget]\n",
    "  end\n",
    "\n",
    "  %% -------- TESTS (HOW WE VERIFY) --------\n",
    "  subgraph TST[\"Tests / Evaluations\"]\n",
    "    T1[Eval-Fact:\\nPF/SK batch judge\\n(extractive + LLM-as-judge)]\n",
    "    T2[Eval-Cite:\\nCoverage + link validity]\n",
    "    T3[Perf-Load:\\nRPS ramp; p95, p99]\n",
    "    T4[Chaos-Retry:\\nBreaker, backoff, errors]\n",
    "    T5[Cost-Meter:\\nper-intent token caps]\n",
    "    T6[Budget-Guard:\\ncheaper-model fallback]\n",
    "  end\n",
    "\n",
    "  %% -------- EVIDENCE / REPORTING --------\n",
    "  subgraph OBS[\"Evidence & Reporting\"]\n",
    "    DSH[Dashboard:\\nweekly trendlines]\n",
    "    RUNS[(Eval run store)]\n",
    "    TRC[(Per-turn traces)]\n",
    "    GATE[CI Gate:\\nblock on regression]\n",
    "  end\n",
    "\n",
    "  %% -------- TRACEABILITY LINKS --------\n",
    "  R1 --> M1\n",
    "  R1 --> M1\n",
    "  R1 --> T1\n",
    "  R1 --> T2\n",
    "\n",
    "  R2 --> M2\n",
    "  R2 --> T3\n",
    "  R2 --> T4\n",
    "\n",
    "  R3 --> M3\n",
    "  R3 --> T5\n",
    "  R3 --> T6\n",
    "\n",
    "  %% Metrics feed tests & evidence\n",
    "  T1 --> RUNS --> DSH\n",
    "  T2 --> RUNS\n",
    "  T3 --> TRC --> DSH\n",
    "  T4 --> TRC\n",
    "  T5 --> TRC\n",
    "  T6 --> TRC\n",
    "\n",
    "  %% Release control\n",
    "  DSH --> GATE\n",
    "\n",
    "  %% ---------------- NOTES ----------------\n",
    "  %% - Update thresholds per tenant/intent in config-as-code.\n",
    "  %% - Gate deploys on: (Factuality < 0.90) OR (p95 > SLO) OR ($/turn > budget).\n",
    "\n",
    "  %% ---------------- STYLES ----------------\n",
    "  classDef req fill:#e8f0fe,stroke:#1a73e8,stroke-width:2px,color:#0b468c;\n",
    "  classDef met fill:#fef9c3,stroke:#f59e0b,stroke-width:2px,color:#7c2d12;\n",
    "  classDef tst fill:#ecfeff,stroke:#06b6d4,stroke-width:2px,color:#134e4a;\n",
    "  classDef obs fill:#f5f3ff,stroke:#8b5cf6,stroke-width:2px,color:#4c1d95;\n",
    "\n",
    "  class REQ,R1,R2,R3 req\n",
    "  class MET,M1,M2,M3 met\n",
    "  class TST,T1,T2,T3,T4,T5,T6 tst\n",
    "  class OBS,DSH,RUNS,TRC,GATE obs\n",
    "`;\n",
    "  var s = document.createElement('script');\n",
    "  s.src = \"https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js\";\n",
    "  s.onload = function() { window.mermaid.initialize({ startOnLoad: true, securityLevel: 'loose' }); };\n",
    "  document.body.appendChild(s);\n",
    "})();\n",
    "</script>\n",
    "\"\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4ae6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [SETUP-ENV]\n",
    "import os, getpass\n",
    "os.environ.setdefault('AZURE_OPENAI_ENDPOINT', 'https://4th-openai-resource.openai.azure.com')\n",
    "os.environ.setdefault('AZURE_OPENAI_DEPLOYMENT', 'gpt-35-turbo')\n",
    "os.environ.setdefault('AZURE_OPENAI_API_VERSION', '2024-10-21')\n",
    "if not os.getenv('AZURE_OPENAI_API_KEY'):\n",
    "    try:\n",
    "        os.environ['AZURE_OPENAI_API_KEY'] = getpass.getpass('Enter AZURE_OPENAI_API_KEY (hidden): ').strip()\n",
    "    except Exception:\n",
    "        # Non-interactive context\n",
    "        os.environ['AZURE_OPENAI_API_KEY'] = 'stub-key'\n",
    "print('Azure OpenAI env ready (key is session-only).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f53a214",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [KERNEL]\n",
    "class MiniKernel:\n",
    "    async def invoke_prompt(self, text: str):\n",
    "        # Deteministic stubbed reply\n",
    "        return {\"message\": \"[stub-llm] ok: \" + (text[:120] + (\"...\" if len(text) > 120 else \"\"))}\n",
    "\n",
    "kernel = MiniKernel()\n",
    "print(\"Kernel ready (stubbed).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f092e7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [TOOLS]\n",
    "import random\n",
    "\n",
    "def tool_accuracy_eval(expected=0.90, cite_cov=0.85, **kwargs) -> str:\n",
    "    # simple stub with random-ish values\n",
    "    fac = round(random.uniform(0.88, 0.98), 3)\n",
    "    cov = round(random.uniform(0.82, 0.92), 3)\n",
    "    status = \"pass\" if fac >= expected and cov >= cite_cov else \"warn\"\n",
    "    return f\"stub:accuracy fac={fac} cov={cov} target=({expected},{cite_cov}) status={status}\"\n",
    "\n",
    "def tool_latency_probe(p95_target_ms=1200, **kwargs) -> str:\n",
    "    p95 = random.randint(900, 1600)\n",
    "    status = \"pass\" if p95 <= p95_target_ms else \"warn\"\n",
    "    return f\"stub:latency p95={p95}ms target<={p95_target_ms} status={status}\"\n",
    "\n",
    "def tool_cost_meter(budget_usd=0.012, **kwargs) -> str:\n",
    "    cost = round(random.uniform(0.006, 0.018), 4)\n",
    "    status = \"pass\" if cost <= budget_usd else \"warn\"\n",
    "    return f\"stub:cost ${cost}/turn budget<={budget_usd} status={status}\"\n",
    "\n",
    "TOOLS = {\n",
    "    \"accuracy_eval\": tool_accuracy_eval,\n",
    "    \"latency_probe\": tool_latency_probe,\n",
    "    \"cost_meter\": tool_cost_meter,\n",
    "}\n",
    "print(\"Tools:\", \", \".join(sorted(TOOLS.keys())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427850c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [AGENTS]\n",
    "class BaseAgent:\n",
    "    name = \"Base\"\n",
    "    skills = []\n",
    "    def __init__(self, kernel): self.kernel = kernel\n",
    "    def available_tools(self): \n",
    "        return [t for t in self.skills if t in TOOLS]\n",
    "    def call(self, tool_name: str, **kwargs):\n",
    "        fn = TOOLS.get(tool_name)\n",
    "        if not fn: raise ValueError(f\"Tool not found: {tool_name}\")\n",
    "        return fn(**kwargs)\n",
    "    async def run(self, user_text: str) -> str:\n",
    "        try:\n",
    "            out = await self.kernel.invoke_prompt(user_text)\n",
    "            return out[\"message\"]\n",
    "        except Exception as e:\n",
    "            return f\"[agent:{self.name}] LLM call failed: {e}\"\n",
    "\n",
    "class Agent_PVA_APIM(BaseAgent):\n",
    "    name = \"PVA/APIM (Latency focus)\"\n",
    "    skills = [\"latency_probe\"]\n",
    "\n",
    "class Agent_Orchestrator(BaseAgent):\n",
    "    name = \"Orchestrator (SK/LangGraph)\"\n",
    "    skills = []\n",
    "\n",
    "class Agent_Tools_RAG(BaseAgent):\n",
    "    name = \"Tools / RAG (Accuracy)\"\n",
    "    skills = [\"accuracy_eval\"]\n",
    "\n",
    "class Agent_DataPlane(BaseAgent):\n",
    "    name = \"Data Plane (Residency)\"\n",
    "    skills = []\n",
    "\n",
    "class Agent_Ops(BaseAgent):\n",
    "    name = \"Ops / CI-CD (Cost)\"\n",
    "    skills = [\"cost_meter\"]\n",
    "\n",
    "agent_pva_apim   = Agent_PVA_APIM(kernel)\n",
    "agent_orchestrator = Agent_Orchestrator(kernel)\n",
    "agent_tools_rag  = Agent_Tools_RAG(kernel)\n",
    "agent_data_plane = Agent_DataPlane(kernel)\n",
    "agent_ops        = Agent_Ops(kernel)\n",
    "\n",
    "print(\"Agents:\", [a.name for a in [\n",
    "    agent_pva_apim, agent_orchestrator, agent_tools_rag, agent_data_plane, agent_ops\n",
    "]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f3cad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [WIRES]\n",
    "ROUTES = {\n",
    "    \"REQ-001: Grounded answers are accurate\": \"tools_rag\",\n",
    "    \"REQ-002: Fast responses at scale\": \"pva_apim\",\n",
    "    \"REQ-003: Cost efficiency per turn\": \"ops\",\n",
    "}\n",
    "AGENT_INDEX = {\n",
    "    \"pva_apim\": agent_pva_apim,\n",
    "    \"orchestrator\": agent_orchestrator,\n",
    "    \"tools_rag\": agent_tools_rag,\n",
    "    \"data_plane\": agent_data_plane,\n",
    "    \"ops\": agent_ops,\n",
    "}\n",
    "\n",
    "def validate_wiring():\n",
    "    problems = []\n",
    "    for req, key in ROUTES.items():\n",
    "        agent = AGENT_INDEX.get(key)\n",
    "        if not agent:\n",
    "            problems.append(f\"{req} -> missing agent key '{key}'\")\n",
    "            continue\n",
    "        if not agent.available_tools():\n",
    "            # not all agents must have tools for this matrix; still report\n",
    "            problems.append(f\"{req} -> {agent.name} has no available tools\")\n",
    "    return problems\n",
    "\n",
    "total_wires = len(ROUTES)\n",
    "distinct_agents = len(set(ROUTES.values()))\n",
    "unreferenced_agents = sorted(set(AGENT_INDEX.keys()) - set(ROUTES.values()))\n",
    "targets_by_agent = {}\n",
    "for req, key in ROUTES.items():\n",
    "    targets_by_agent.setdefault(key, []).append(req)\n",
    "\n",
    "issues = validate_wiring()\n",
    "print(f\"Wires: {total_wires} (distinct agents: {distinct_agents})\")\n",
    "for agent_key, reqs in targets_by_agent.items():\n",
    "    agent = AGENT_INDEX.get(agent_key)\n",
    "    agent_name = getattr(agent, \"name\", agent_key)\n",
    "    print(f\"  - {agent_name} ← {len(reqs)} requirement(s): {', '.join(reqs)}\")\n",
    "if unreferenced_agents:\n",
    "    print(f\"Unreferenced agents: {', '.join(unreferenced_agents)}\")\n",
    "print(\"Wiring OK\" if not issues else \"Wiring issues:\\n- \" + \"\\n- \".join(issues))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c677a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [DEMO]\n",
    "import asyncio, time, nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def demo_run():\n",
    "    t0 = time.time()\n",
    "    samples = [\n",
    "        (\"REQ-001\", \"Verify factuality and citations meet thresholds.\"),\n",
    "        (\"REQ-002\", \"Check p95 latency is under SLO during load.\"),\n",
    "        (\"REQ-003\", \"Ensure $/turn is within budget for this intent.\"),\n",
    "    ]\n",
    "    outputs = []\n",
    "    for rid, text in samples:\n",
    "        if rid == \"REQ-001\":\n",
    "            agent = AGENT_INDEX[\"tools_rag\"]\n",
    "            tool_out = agent.call(\"accuracy_eval\", expected=0.90, cite_cov=0.85)\n",
    "        elif rid == \"REQ-002\":\n",
    "            agent = AGENT_INDEX[\"pva_apim\"]\n",
    "            tool_out = agent.call(\"latency_probe\", p95_target_ms=1200)\n",
    "        else:\n",
    "            agent = AGENT_INDEX[\"ops\"]\n",
    "            tool_out = agent.call(\"cost_meter\", budget_usd=0.012)\n",
    "\n",
    "        llm_out = await agent.run(text)\n",
    "        outputs.append({\n",
    "            \"req\": rid,\n",
    "            \"agent\": agent.name,\n",
    "            \"tool_result\": tool_out,\n",
    "            \"llm_result\": llm_out[:220] + (\"...\" if len(llm_out) > 220 else \"\")\n",
    "        })\n",
    "    elapsed_ms = int((time.time() - t0)*1000)\n",
    "    return {\"elapsed_ms\": elapsed_ms, \"runs\": outputs}\n",
    "\n",
    "try:\n",
    "    loop = asyncio.get_running_loop()\n",
    "    result = loop.run_until_complete(demo_run())\n",
    "except RuntimeError:\n",
    "    result = asyncio.run(demo_run())\n",
    "\n",
    "print(\"Elapsed (ms):\", result[\"elapsed_ms\"])\n",
    "for r in result[\"runs\"]:\n",
    "    print(f\"REQ: {r['req']} -> Agent: {r['agent']}\")\n",
    "    print(f\"  Tool: {r['tool_result']}\")\n",
    "    print(f\"  LLM:  {r['llm_result']}\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
