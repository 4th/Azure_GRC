{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Assembled Notebook — SK Agents\n"
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "SETUP"
        ]
      },
      "source": "# %% [SETUP]\n# Package install (unpin; upgrade SK)\n!pip install -U semantic-kernel\n!pip -q uninstall -y pydrive2\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "SETUP-ENV"
        ]
      },
      "source": "# %% [SETUP-ENV]\n# Environment variables for Azure OpenAI (replace placeholder strings with real values)\nimport os, getpass\n\nos.environ.setdefault(\"AZURE_OPENAI_ENDPOINT\",    \"https://YOUR-AOAI.openai.azure.com\")\nos.environ.setdefault(\"AZURE_OPENAI_API_KEY\",     \"\")\nos.environ.setdefault(\"AZURE_OPENAI_API_VERSION\", \"2024-10-21\")\nos.environ.setdefault(\"AZURE_OPENAI_DEPLOYMENT\",  \"gpt-35-turbo\")  # your deployment name\n\nif not os.getenv(\"AZURE_OPENAI_API_KEY\"):\n    try:\n        os.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass(\"Enter AZURE_OPENAI_API_KEY (hidden): \").strip()\n    except Exception:\n        pass\n\nprint(\"AZURE_OPENAI_ENDPOINT :\", os.getenv(\"AZURE_OPENAI_ENDPOINT\"))\nprint(\"AZURE_OPENAI_DEPLOYMENT:\", os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"))\nprint(\"API key set?           :\", \"yes\" if bool(os.getenv(\"AZURE_OPENAI_API_KEY\")) else \"no\")\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "KERNEL"
        ]
      },
      "source": "# %% [KERNEL]\nimport os\nfrom semantic_kernel import Kernel\n\n# Preferred import per your instruction\ntry:\n    from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n    _azure_cls = AzureChatCompletion\n    _use_shim = False\nexcept Exception as e:\n    # Fallback shim to remain runnable across SK versions\n    from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion as AzureChatCompletion  # type: ignore\n    _azure_cls = AzureChatCompletion\n    _use_shim = True\n\nkernel = Kernel()\n\nservice = _azure_cls(\n    service_id=\"azure\",\n    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n    deployment_name=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),  # AOAI deployment name\n    endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n    # api_version can be set explicitly if needed:\n    # api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-10-21\")\n)\n\nkernel.add_service(service)\nprint(\"Kernel ready (Azure OpenAI) | shim:\", _use_shim)\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "TOOLS"
        ]
      },
      "source": "# %% [TOOLS]\n# Tools map to the pipeline: RAG packet build, execution, metering, etc.\n# These are minimal, safe stubs you can later replace with real integrations.\n\nimport time\nfrom typing import Any, Dict, List\n\ndef tool_pf_run(config: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Batch runner stub (Promptflow/SK).\"\"\"\n    return {\"status\": \"ok\", \"batches\": 1, \"config\": dict(config)}\n\ndef tool_build_rag_packet(query: str, k: int = 4) -> Dict[str, Any]:\n    \"\"\"Hybrid retrieval stub that returns a fake grounding packet & citations.\"\"\"\n    cites = [{\"doc\": \"sk://stub/doc1\", \"chunk\": 1}, {\"doc\": \"sk://stub/doc2\", \"chunk\": 3}]\n    return {\"query\": query, \"topk\": k, \"citations\": cites, \"context\": \"stubbed grounding packet\"}\n\ndef tool_exec_llm(prompt: str, need_citations: bool = False) -> Dict[str, Any]:\n    \"\"\"Minimal LLM exec using kernel.invoke_prompt if available; falls back to echo.\"\"\"\n    try:\n        if hasattr(kernel, \"invoke_prompt\"):\n            import asyncio\n            async def _call():\n                resp = await kernel.invoke_prompt(prompt)\n                return str(resp)\n            text = asyncio.get_event_loop().run_until_complete(_call())\n        else:\n            text = \"[no kernel.invoke_prompt] \" + prompt[:200]\n    except Exception as e:\n        text = f\"[exec error] {e} | prompt: {prompt[:160]}\"\n    cites = [{\"doc\": \"sk://stub/doc1\"}] if need_citations else []\n    return {\"answer\": text, \"citations\": cites}\n\ndef tool_meter(tokens_in: int, tokens_out: int, latency_ms: int) -> Dict[str, Any]:\n    return {\"tokens_in\": tokens_in, \"tokens_out\": tokens_out, \"latency_ms\": latency_ms, \"cost_usd\": round((tokens_in+tokens_out)/1e6, 6)}\n\nTOOLS = {\n    \"pf_run\": tool_pf_run,\n    \"rag_packet\": tool_build_rag_packet,\n    \"exec_llm\": tool_exec_llm,\n    \"meter\": tool_meter,\n}\nprint(\"Tools ready:\", list(TOOLS.keys()))\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "AGENTS"
        ]
      },
      "source": "# %% [AGENTS]\nimport asyncio\n\nclass AgentPlanner:\n    def __init__(self, kernel):\n        self.kernel = kernel\n        self.name = \"Planner\"\n        self.system = \"You plan eval runs and select steps based on config.\"\n\n    async def run(self, cfg: dict) -> dict:\n        # Simple plan: always run PF, build RAG, then EXEC\n        return {\"steps\": [\"pf_run\", \"rag_packet\", \"exec\"], \"budget_tokens\": 20000}\n\nclass AgentExecutor:\n    def __init__(self, kernel):\n        self.kernel = kernel\n        self.name = \"Executor\"\n\n    async def run(self, prompt: str, citations: bool = False) -> dict:\n        return TOOLS[\"exec_llm\"](prompt, need_citations=citations)\n\nclass AgentMetrics:\n    def factuality(self, expected_list, answer_text):\n        if not expected_list:\n            return 0.5\n        t = (answer_text or \"\").lower()\n        return 1.0 if all(s.lower() in t for s in expected_list) else 0.0\n    def citation(self, required, citations):\n        return 1.0 if (not required or citations) else 0.0\n    def latency(self, start_ns=None, end_ns=None):\n        if start_ns and end_ns:\n            return max(0, int((end_ns - start_ns)/1e6))\n        return 0\n\nclass AgentStorage:\n    async def save(self, results):\n        # TODO: Persist to blob/db if desired\n        return {\"saved\": len(results)}\n\nclass AgentAlerts:\n    async def notify(self, alert):\n        # TODO: Wire to Teams/Email if desired\n        print(\"[ALERT]\", alert)\n        return True\n\nclass AgentDashboard:\n    async def publish(self, results):\n        # TODO: Push to dashboard/wandb/etc.\n        return {\"published\": len(results)}\n\nagent_planner   = AgentPlanner(kernel)\nagent_exec      = AgentExecutor(kernel)\nagent_metrics   = AgentMetrics()\nagent_storage   = AgentStorage()\nagent_alerts    = AgentAlerts()\nagent_dashboard = AgentDashboard()\n\nprint(\"Agents:\", [a for a in [\"agent_planner\",\"agent_exec\",\"agent_metrics\",\"agent_storage\",\"agent_alerts\",\"agent_dashboard\"]])\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "WIRES"
        ]
      },
      "source": "# %% [WIRES]\n# Placeholder wiring; expand as needed.\nWIRES = {\n    \"planner->pf_run\": True,\n    \"planner->rag_packet\": True,\n    \"planner->exec\": True,\n    \"exec->meter\": True,\n}\nprint(\"Wires:\", WIRES)\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "PATCH"
        ]
      },
      "source": "# %% [PATCH: robust eval + normalization]\nimport time\nfrom typing import Any, Dict, List\n\ndef _normalize_item(item: Any) -> Dict[str, Any]:\n    if isinstance(item, str):\n        return {\"prompt\": item, \"expected_contains\": [], \"citations_required\": False, \"refusal_expected\": False}\n    d = dict(item)\n    d[\"prompt\"] = d.get(\"prompt\") or d.get(\"input\") or d.get(\"query\") or d.get(\"text\") or \"\"\n    exp = d.get(\"expected_contains\", d.get(\"expected\", d.get(\"must_include\", [])))\n    if exp is None: exp = []\n    if isinstance(exp, str): exp = [exp]\n    d[\"expected_contains\"] = list(exp)\n    d[\"citations_required\"] = bool(d.get(\"citations_required\") or d.get(\"needs_citation\") or d.get(\"citation\", False))\n    d[\"refusal_expected\"]   = bool(d.get(\"refusal_expected\") or d.get(\"should_refuse\") or d.get(\"refusal\", False))\n    return d\n\ndef _coerce_answer(ans: Any) -> Dict[str, Any]:\n    if isinstance(ans, dict):\n        text = ans.get(\"answer\", ans.get(\"text\", \"\"))\n        cites = ans.get(\"citations\", ans.get(\"cites\", [])) or []\n        if isinstance(cites, dict): cites = list(cites.values())\n        return {\"answer\": str(text), \"citations\": list(cites)}\n    return {\"answer\": str(ans), \"citations\": []}\n\nasync def eval_nightly():\n    t0 = time.time_ns()\n    # Try to read preloaded TESTCASES; fallback to one case\n    tc_raw = globals().get(\"TESTCASES\") or [\n        {\"prompt\": \"Summarize the eval pipeline in one sentence.\", \"expected_contains\": [\"eval\",\"metrics\"], \"citations_required\": False}\n    ]\n    tc = [_normalize_item(x) for x in tc_raw]\n\n    results = []\n    alerts = []\n    for item in tc:\n        start_ns = time.time_ns()\n        out = await agent_exec.run(item[\"prompt\"], citations=item[\"citations_required\"])\n        ans = _coerce_answer(out)\n        end_ns = time.time_ns()\n\n        fact = agent_metrics.factuality(item.get(\"expected_contains\", []), ans[\"answer\"])\n        cite = agent_metrics.citation(item.get(\"citations_required\", False), ans.get(\"citations\", []))\n        lat  = agent_metrics.latency(start_ns, end_ns)\n\n        row = {\n            \"prompt\": item[\"prompt\"],\n            \"answer\": ans[\"answer\"],\n            \"citations\": ans.get(\"citations\", []),\n            \"factuality\": fact,\n            \"citation_ok\": cite,\n            \"latency_ms\": lat,\n        }\n        results.append(row)\n\n        if fact < 0.5 or (item.get(\"citations_required\", False) and not cite):\n            alerts.append({\n                \"prompt\": item[\"prompt\"][:160],\n                \"reason\": \"low_factuality\" if fact < 0.5 else \"missing_citations\",\n                \"metrics\": {\"factuality\": fact, \"citation_ok\": cite, \"latency_ms\": lat},\n            })\n\n    # Optional publish\n    try:\n        await agent_storage.save(results)\n        await agent_dashboard.publish(results)\n        for a in alerts: await agent_alerts.notify(a)\n    except Exception:\n        pass\n\n    elapsed_ms = int((time.time_ns()-t0)/1e6)\n    return {\n        \"summary\": {\n            \"total\": len(results),\n            \"alerts\": len(alerts),\n            \"avg_latency_ms\": int(sum(r[\"latency_ms\"] for r in results)/max(1,len(results))),\n            \"avg_factuality\": round(sum(r[\"factuality\"] for r in results)/max(1,len(results)), 3),\n        },\n        \"alerts\": alerts,\n        \"actions\": [\"open_pr_prompts\"] if alerts else [],\n        \"elapsed_ms\": elapsed_ms,\n        \"results\": results,\n    }\nprint(\"Patched eval_nightly ready.\")\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "DEMO"
        ]
      },
      "source": "# %% [DEMO]\nimport time, asyncio\nt0 = time.time()\n\n# In Jupyter, a loop is already running; use top-level await\nresult = await eval_nightly()\n\nprint(\"Eval summary:\", result[\"summary\"])\nprint(\"Alerts:\", result[\"alerts\"])\nprint(\"Actions:\", result[\"actions\"])\nprint(\"Elapsed (ms):\", int((time.time()-t0)*1000))\n\n# Peek at first 3 results\nfor i, r in enumerate(result[\"results\"][:3], 1):\n    print(f\"\\nCase {i}\")\n    print(\" prompt   :\", (r[\"prompt\"][:120] + (\"…\" if len(r[\"prompt\"])>120 else \"\")))\n    print(\" factual  :\", r[\"factuality\"], \" citation_ok:\", r[\"citation_ok\"], \" latency_ms:\", r[\"latency_ms\"])\n    print(\" answer   :\", (r[\"answer\"][:200] + (\"…\" if len(r[\"answer\"])>200 else \"\")))\n",
      "outputs": [],
      "execution_count": null
    }
  ]
}